{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure you successfully run and test a few models in __llm_testing.ipynb__ before implementing a RAG pattern**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### RAG Pattern Overview \n",
    "\n",
    "RAG stands for Retrieval Augmented Generation. A RAG pattern is essentially a way to augment our models response by grounding it in some sort of information retrieved from a database (typically a chunk of a document). \n",
    "\n",
    "A general flow for the RAG pattern is; \n",
    "1. A user enters a question or a prompt \n",
    "2. The users query is searched for in a vector database \n",
    "3. The documents retrieved from the vector database are passed to GenAI alongside a prompt\n",
    "    i. (for example, \"Your job is to summarize the chunks of the documents\")\n",
    "4. Our generative AI provides us a response grounded in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "## DO NOT SHARE .env ANYWHERE\n",
    "load_dotenv()\n",
    "project_id = os.getenv('PROJECT_ID')\n",
    "api_key=os.getenv('GENAI_API_KEY')\n",
    "url=os.getenv('GENAI_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it is made up of different wavelengths, including red, orange, yellow, green, blue, and violet. Shorter-wavelength light, such as blue and violet, is scattered in all directions more than longer-wavelength light. As a result, the scattered blue and violet light reach our eyes from all directions, making the sky appear blue. This scattering also causes the sun to appear yellow during sunrise and sunset, as the shorter-wavelength light is scattered away from our line of sight, leaving the longer-wavelength light, like red and orange, to dominate.\n"
     ]
    }
   ],
   "source": [
    "# Modify your prompt below. \n",
    "prompt = '''\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Why is the sky blue?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Tokens can be thought of as part words - almost like syllables. \n",
    "# Larger MAX_NEW_TOKENS will allow the models to generate longer responses.\n",
    "# Each model has a unique context window for tokens - for llama3 it is 8000 tokens. \n",
    "generate_params = {\n",
    "            GenParams.MAX_NEW_TOKENS: 300\n",
    "        }\n",
    "\n",
    "## Model choices:\n",
    "# - ibm/granite-34b-code-instruct\n",
    "# - meta-llama/llama-3-70b-instruct\n",
    "# - ibm/granite-8b-code-instruct\n",
    "# - meta-llama/llama-3-8b-instruct\n",
    "# - ibm/granite-13b-chat-v2\n",
    "\n",
    "model = Model(\n",
    "    model_id=\"ibm/granite-13b-chat-v2\",\n",
    "    params=generate_params,\n",
    "    credentials={\n",
    "        \"apikey\": f\"{api_key}\",\n",
    "        \"url\": f\"{url}\"\n",
    "    },\n",
    "    project_id=project_id\n",
    "    )\n",
    "\n",
    "\n",
    "## For testing\n",
    "# generated_response = model.generate(prompt=prompt)\n",
    "# print(generated_response['results'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Set up vector DB\n",
    "\n",
    "- The vector DB will store all your documents after they are chunked \n",
    "- It will be searched by the LLM to find appropriate responses to queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "## We will be using ChromaDB but feel free to try other vector DBs such as FAISS, Milvus, ElasticSearch...etc.\n",
    "\n",
    "# Instantiate database client \n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection to store the vectors (or get collection if it already exists)\n",
    "# NOTE: If you want to test with a fresh collection every time, use delete_collection\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create embeddings\n",
    "\n",
    "- Embeddings are vectors associated with tokens from a text\n",
    "- An embedding function will take text chunks as input and output vectors to represent the tokens\n",
    "- Vectors are fed into the vector DB\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Read pdf / document\n",
    "2. Chunk document \n",
    "3. Feed chunks to embedding function\n",
    "4. Put embeddings into vector DB\n",
    "\n",
    "**Lines for you to code are marked with <-- TODO -->**\n",
    "\n",
    "**All TODOs can be done with the existing imports. Read the documentation and resources to find what functions to use. If you would like to do your own implementation or use other libraries, feel free**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australia_code_vol_1.pdf', 'Australia_code_vol_2.pdf', 'Australia_code_vol_3.pdf', 'Australia_housing_provisions.pdf', 'Building Control Act 1989 Singapore.pdf', 'Building Control Regulations 2003 Singapore.pdf', 'HK_demolition.pdf', 'HK_escalators.pdf', 'HK_external_maintenance.pdf', 'HK_firefighting.pdf', 'HK_fire_escape.pdf', 'HK_fire_resistance.pdf', 'HK_fire_safety.pdf', 'HK_foundations.pdf', 'HK_glass.pdf', 'HK_site_supervision.pdf', 'HK_steel.pdf', 'HK_thermal_transfer.pdf', 'HK_wind.pdf', 'Japan_building_standard_law.pdf', 'London_building_act.pdf', 'London_building_regulations.pdf', 'Los Angeles County, CA Code of Ordinances.pdf', 'Netherlands-2011-0212-000-EN.pdf', 'NYC_local_laws_2023.pdf', 'Paris_regulations.pdf', 'singapore_building_control_buildability_regulations.pdf', 'singapore_building_control_env_sus_regulations.pdf', 'singapore_circular2017.pdf', 'singapore_cop2017.pdf', 'Toronto_Building Code Act, 1992, S.O. 1992, c. 23.pdf', 'Toronto_O. Reg. 332_12_ BUILDING CODE.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "## Set up embedding function\n",
    "# embedding params\n",
    "embed_params = {\n",
    "            EmbedParams.TRUNCATE_INPUT_TOKENS: 3,\n",
    "            EmbedParams.RETURN_OPTIONS: {\n",
    "            'input_text': True\n",
    "            }\n",
    "        }\n",
    "\n",
    "# embedding function\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=f\"{url}\",\n",
    "    apikey=f\"{api_key}\",\n",
    "    project_id=project_id,\n",
    "    params=embed_params,\n",
    ")\n",
    "\n",
    "### Extract text from pdfs\n",
    "## 1. Parse PDFs \n",
    "\n",
    "test_file = \"da-vinci-test.pdf\"  # File to test your rag pattern\n",
    "\n",
    "all_files = os.listdir(path='./pdfs')  # List of all PDF files to use in your implementation (construction codes)\n",
    "\n",
    "\n",
    "## Load the pdf file (start with test file)\n",
    "\n",
    "# <-- TODO: Create loader -->\n",
    "\n",
    "document = loader.load()  # loads file from loader\n",
    "\n",
    "\n",
    "## 2. chunk text \n",
    "\n",
    "## Split the text\n",
    "# <-- TODO: Create text splitter -->\n",
    "\n",
    "# <-- TODO: Use text splitter to chunk 'document' -->\n",
    "\n",
    "\n",
    "\n",
    "## 3. create embeddings and store in vector DB\n",
    "\n",
    "# Create embeddings and store in collection\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=#<-- TODO: Put appropriate var -->,\n",
    "    embedding=#<-- TODO: Put appropriate var -->,\n",
    "    collection_name=\"my_collection\",\n",
    "    client=chroma_client,\n",
    ")\n",
    "\n",
    "\n",
    "#### TODO: Modify steps 1 to 3 so that it works for all_files\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Combine elements into RAG pattern\n",
    "\n",
    "RAG patterns consist of:\n",
    "- A query\n",
    "- A model to answer the query\n",
    "- An engineered prompt for the model which asks for the query to be answered using the provided context \n",
    "- A retriever to retrieve the appropriate documents from the vector DB using the original query\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Query is asked\n",
    "2. Query is input to the engineered prompt\n",
    "3. Engineered prompt is asked to the model\n",
    "4. Vector DB is searched to find best answer from the provided context (done by retriever)\n",
    "5. Vector DB returns new information to the model\n",
    "6. Model generates answer to original query using the information from the vector DB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to combine list of docs from vector DB into a single doc\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Function to combine all elements into a complete RAG pattern\n",
    "# Returns RAG response from LLM\n",
    "def rag_query(query, retriever, model)->str:\n",
    "    \n",
    "    # Template for engineered prompt asking for a context-based answer\n",
    "    template = f\"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {query} \n",
    "        Context: {format_docs(retriever.invoke(query))} \n",
    "        Answer:\n",
    "        \"\"\"   \n",
    "    \n",
    "    \n",
    "    ## Generate a context based response from the LLM\n",
    "    ## <-- TODO: Generate a response by feeding the engineered prompt to the LLM-->\n",
    "    ## <-- TODO: Return response string -->\n",
    "\n",
    "\n",
    "### Run your implemented RAG pattern!\n",
    "\n",
    "# Instantiates a retriever from the vector DB to find appropriate context\n",
    "retriever = chroma_db.as_retriever()\n",
    "\n",
    "# Sample question for test pdf\n",
    "# Verify the answer by reading page 3 in the test pdf\n",
    "question = \"What happened to art during the Renaissance?\"\n",
    "response = rag_query(question, retriever, model)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONGRATULATIONS! ###\n",
    "\n",
    "You have created your first RAG pattern (or maybe you've done this before)! Now take this knowledge and see where you can take it to solve the brief. Good luck :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
